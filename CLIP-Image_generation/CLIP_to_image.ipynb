{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-clip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27cyvXp3zYea",
        "outputId": "250cf90f-706b-4a4b-b946-7c19925142a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-clip in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from openai-clip) (6.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from openai-clip) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-clip) (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->openai-clip) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import clip\n",
        "from tqdm import tqdm\n",
        "\n",
        "class StyleTransfer:\n",
        "    def __init__(self, clip_model_name=\"ViT-B/32\"):\n",
        "        # Set device\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Load CLIP model\n",
        "        self.model, self.preprocess = clip.load(clip_model_name, device=self.device)\n",
        "\n",
        "        # Freeze the CLIP model parameters\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def get_text_features(self, text):\n",
        "        \"\"\"Get CLIP text features for style\"\"\"\n",
        "        text_tokens = clip.tokenize([text]).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            text_features = self.model.encode_text(text_tokens)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "        return text_features\n",
        "\n",
        "    def transfer_style(self,\n",
        "                      content_image_path,\n",
        "                      style_text,\n",
        "                      num_steps=300,\n",
        "                      lr=0.5,\n",
        "                      content_weight=0.0,\n",
        "                      style_weight=1.0,\n",
        "                      tv_weight=0.001):\n",
        "        \"\"\"Perform style transfer\"\"\"\n",
        "        # Load and preprocess content image\n",
        "        content_pil = Image.open(content_image_path).convert(\"RGB\")\n",
        "\n",
        "        # Get original size for later\n",
        "        original_size = content_pil.size\n",
        "\n",
        "        # Process for CLIP input (resize to what CLIP expects)\n",
        "        content_clip = self.preprocess(content_pil).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # IMPORTANT: Create a fresh tensor for optimization, detached from computation graph\n",
        "        # Start with random noise and then add content\n",
        "        opt_img = torch.randn(content_clip.shape, device=self.device) * 0.1\n",
        "        # Add content image (without gradients)\n",
        "        with torch.no_grad():\n",
        "            opt_img = torch.clamp(opt_img + content_clip.detach(), 0, 1)\n",
        "        # Make sure it's a leaf tensor requiring gradients\n",
        "        opt_img.requires_grad_(True)\n",
        "\n",
        "        # Get text features for style\n",
        "        text_features = self.get_text_features(style_text)\n",
        "\n",
        "        # Get content features (if needed for content preservation)\n",
        "        if content_weight > 0:\n",
        "            with torch.no_grad():\n",
        "                content_features = self.model.encode_image(content_clip)\n",
        "                content_features = content_features / content_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Setup optimizer with higher learning rate\n",
        "        optimizer = optim.Adam([opt_img], lr=lr)\n",
        "\n",
        "        # Use a scheduler for better convergence\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_steps)\n",
        "\n",
        "        # Track best result\n",
        "        best_loss = float('inf')\n",
        "        best_img = None\n",
        "\n",
        "        # Main optimization loop\n",
        "        for i in tqdm(range(num_steps)):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get current image features\n",
        "            image_features = self.model.encode_image(opt_img)\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # Style loss - cosine distance to style text embedding\n",
        "            style_loss = style_weight * (1 - torch.cosine_similarity(image_features, text_features))\n",
        "\n",
        "            # Content loss - if requested\n",
        "            if content_weight > 0:\n",
        "                content_loss = content_weight * (1 - torch.cosine_similarity(image_features, content_features))\n",
        "            else:\n",
        "                content_loss = torch.tensor(0.0, device=self.device)\n",
        "\n",
        "            # Total variation regularization for smoothness\n",
        "            diff_y = torch.abs(opt_img[:, :, :-1, :] - opt_img[:, :, 1:, :])\n",
        "            diff_x = torch.abs(opt_img[:, :, :, :-1] - opt_img[:, :, :, 1:])\n",
        "            tv_loss = tv_weight * (torch.sum(diff_x) + torch.sum(diff_y))\n",
        "\n",
        "            # Combine losses\n",
        "            total_loss = style_loss + content_loss + tv_loss\n",
        "\n",
        "            # Backward pass\n",
        "            total_loss.backward()\n",
        "\n",
        "            # Check gradients - helpful for debugging\n",
        "            if i == 0:\n",
        "                grad_norm = opt_img.grad.norm().item()\n",
        "                print(f\"Gradient norm: {grad_norm}\")\n",
        "                if grad_norm < 1e-4:\n",
        "                    print(\"WARNING: Very small gradients. Trying higher learning rate or different model.\")\n",
        "\n",
        "            # Update image\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Clamp values to valid image range\n",
        "            with torch.no_grad():\n",
        "                opt_img.data.clamp_(0, 1)\n",
        "\n",
        "            # Track best result\n",
        "            if total_loss.item() < best_loss:\n",
        "                best_loss = total_loss.item()\n",
        "                best_img = opt_img.clone().detach()\n",
        "\n",
        "            # Print progress\n",
        "            if i % 20 == 0 or i == num_steps - 1:\n",
        "                print(f\"Step {i}, Style: {style_loss.item():.4f}, Content: {content_loss.item():.4f}, TV: {tv_loss.item():.4f}, Total: {total_loss.item():.4f}\")\n",
        "\n",
        "        # Use best result\n",
        "        final_img = best_img if best_img is not None else opt_img.detach()\n",
        "\n",
        "        # Convert to PIL and resize back to original dimensions\n",
        "        to_pil = transforms.ToPILImage()\n",
        "        result = to_pil(final_img.squeeze().cpu())\n",
        "\n",
        "        # Resize back to original size if needed\n",
        "        if result.size != original_size:\n",
        "            result = result.resize(original_size, Image.LANCZOS)\n",
        "\n",
        "        return result\n",
        "\n",
        "# Example usage\n",
        "def main(save_image = \"imaged\"):\n",
        "    styler = StyleTransfer()\n",
        "    content_path = \"face.jpg\"\n",
        "    style_text = \"Woman standing in a red dress, blue hair.\"\n",
        "\n",
        "    result = styler.transfer_style(\n",
        "        content_path,\n",
        "        style_text,\n",
        "        num_steps=100,\n",
        "        lr=1,           # Higher learning rate for better optimization\n",
        "        content_weight=0.0,  # Small content weight to maintain some structure\n",
        "        style_weight=1,    # Emphasis on style\n",
        "        tv_weight=1e-3,      # Total variation for smoothness\n",
        "    )\n",
        "\n",
        "    result.save(f\"{save_image}.jpg\")\n",
        "    print(\"Style transfer complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rAhrQOHzG-A",
        "outputId": "e4125271-9586-41f5-d935-56b533b23cb1"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 5/100 [00:00<00:02, 43.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 0.5797377228736877\n",
            "Step 0, Style: 0.8789, Content: 0.0000, TV: 10.9085, Total: 11.7812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 25/100 [00:00<00:01, 43.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 20, Style: 0.5669, Content: 0.0000, TV: 50.7933, Total: 51.3438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 50/100 [00:01<00:01, 44.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 40, Style: 0.5234, Content: 0.0000, TV: 30.7401, Total: 31.2500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▍   | 64/100 [00:01<00:01, 35.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 60, Style: 0.5059, Content: 0.0000, TV: 17.3518, Total: 17.8594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▍ | 84/100 [00:02<00:00, 33.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 80, Style: 0.4834, Content: 0.0000, TV: 6.6991, Total: 7.1836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:02<00:00, 36.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 99, Style: 0.4512, Content: 0.0000, TV: 1.3135, Total: 1.7646\n",
            "Style transfer complete!\n"
          ]
        }
      ]
    }
  ]
}